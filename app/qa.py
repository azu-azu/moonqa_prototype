# app/qa.py

import os
import json
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema.runnable import RunnableMap
from langchain.prompts import ChatPromptTemplate
from app.config import get_index_path
from app.settings import SCORE_THRESHOLD, OPENAI_MODEL

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
PROMPT_TEMPLATE = ChatPromptTemplate.from_messages([
    (
        "system",
        "ã‚ãªãŸã¯ã€é–¢è¥¿å¼ã®ã‚„ã‚ã‚‰ã‹ã„èªã‚Šå£ã§è©±ã™ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
        "å£èª¿ã¯ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§ã€å„ªã—ãå¯„ã‚Šæ·»ã†ã‚ˆã†ã«è©±ã—ã¦ãã ã•ã„ã€‚\n"
        "æ–‡ä½“ã¯ã€ã§ã™ãƒ»ã¾ã™èª¿ã€ã¯ä½¿ã‚ãšã€å¥ç‚¹ï¼ˆã€‚ï¼‰ã§æ”¹è¡Œã—ã¦ãã ã•ã„ã€‚\n"
        "èª­ç‚¹ï¼ˆã€ï¼‰ã§ã¯æ”¹è¡Œã—ãªã„ã§ãã ã•ã„ã€‚\n"
        "è©©çš„ã™ãã‚‹è¡¨ç¾ã¯é¿ã‘ã€ãµã‚“ã‚ã‚Šã—ãŸé–¢è¥¿å¼ã§ã€ãƒªã‚ºãƒ ã‚ˆã3ã€œ7è¡Œã§ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n"
    ),
    (
        "human",
        "{context}\n\nè³ªå•: {question}"
    )
])

def load_vectorstore():
    embedding = OpenAIEmbeddings()
    return FAISS.load_local(get_index_path(), embedding, allow_dangerous_deserialization=True)

def retrieve_relevant_docs(vectorstore, query):
    docs_and_scores = vectorstore.similarity_search_with_score(query, k=5)
    # ã‚¹ã‚³ã‚¢ãŒå°ã•ã„ï¼ˆè·é›¢ãŒè¿‘ã„ï¼‰ã‚‚ã®ã‚’æ®‹ã™
    filtered = [(doc, score) for doc, score in docs_and_scores if score <= SCORE_THRESHOLD]
    return filtered

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def get_answer(question, vectorstore):
    """
    æŒ‡å®šã•ã‚ŒãŸè³ªå•ã«å¯¾ã—ã¦å›ç­”ã‚’è¿”ã™è»½é‡é–¢æ•°ã€‚
    - sourceã‚„ãƒãƒ£ãƒ³ã‚¯å†…å®¹ã®ç¢ºèªã¯å«ã¾ã‚Œã¾ã›ã‚“ã€‚
    - ãã‚Œã‚‰ã‚’ç¢ºèªã—ãŸã„å ´åˆã¯ manual_vector_check.py ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
    """

    docs_and_scores = retrieve_relevant_docs(vectorstore, question)
    if not docs_and_scores:
        return "ãƒ‡ãƒ¼ã‚¿ã®ä¸­ã«ã€ä»Šå›ã®ç­”ãˆã¯ãªã‹ã£ãŸã¿ãŸã„ã‚„ã‚ã€‚ã”ã‚ã‚“ã‚„ã§ğŸŒ™", []

    docs = [doc for doc, _ in docs_and_scores]
    context = format_docs(docs)

    chain = RunnableMap({
        "context": lambda _: context,
        "question": lambda _: question
    }) | PROMPT_TEMPLATE | ChatOpenAI(model=OPENAI_MODEL, temperature=0)

    result = chain.invoke({})
    sources = ", ".join(sorted(set(doc.metadata.get("source", "?") for doc in docs)))
    full_answer = result.content + "\n\nå‚ç…§å…ƒï¼š" + sources

    return full_answer, docs_and_scores

def append_json_log(question, answer, docs_and_scores):
    log_entry = {
        "question": question,
        "answer": answer,
        "documents": [
            {
                "content": doc.page_content,
                "score": float(score),
                "source": doc.metadata.get("source", "unknown")
            }
            for doc, score in docs_and_scores
        ]
    }
    os.makedirs("logs", exist_ok=True)
    with open("logs/qa_log.jsonl", "a", encoding="utf-8") as f:
        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

# ä½¿ç”¨ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’ Markdownå½¢å¼ã§å‡ºåŠ›
def print_chunk_info_markdown(docs_and_scores):
    print("\n## ğŸ” ä½¿ç”¨ãƒãƒ£ãƒ³ã‚¯æƒ…å ±\n")
    for i, (doc, score) in enumerate(docs_and_scores):
        source = doc.metadata.get("source", "unknown")
        print(f"### Chunk {i+1}")
        print(f"- **Score**: {score:.4f}")
        print(f"- **Source**: {source}")
        print(f"```\n{doc.page_content.strip()[:500]}\n```") # é•·ã™ãã‚‹æœ¬æ–‡ã¯500æ–‡å­—ã¾ã§
        print()
