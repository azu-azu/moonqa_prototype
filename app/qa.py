# app/qa.py

import argparse
import os
import pickle
from dotenv import load_dotenv

from langchain_community.chat_models import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_community.embeddings import OpenAIEmbeddings

# .env ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# --- FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚€ ---
def load_faiss_index():
    vectorstore = FAISS.load_local(
        folder_path="index/faiss_index",
        embeddings=OpenAIEmbeddings(),
        index_name="index",
        allow_dangerous_deserialization=True # ğŸ‘ˆ ã“ã‚ŒãŒå¿…è¦
    )
    return vectorstore

# --- è³ªå•å¿œç­”å‡¦ç† ---
def question_answer(query):
    vectorstore = load_faiss_index()
    retriever = vectorstore.as_retriever()

    llm = ChatOpenAI(
        temperature=0,
        model="gpt-3.5-turbo"
    )

    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever
    )

    return qa.run(query)

# --- Markdownå½¢å¼ã§ãƒ­ã‚°ã‚’è¿½è¨˜ä¿å­˜ ---
def append_markdown_log(question: str, answer: str):
    os.makedirs("logs", exist_ok=True)
    log_path = "logs/qa_log.md"
    with open(log_path, "a", encoding="utf-8") as f:
        f.write(f"\n## ğŸ’¬ Question\n{question}\n\n## ğŸ“– Answer\n{answer}\n\n---\n")

# --- CLIå®Ÿè¡Œ ---
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--question", type=str, required=True, help="è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    args = parser.parse_args()

    answer = question_answer(args.question)
    append_markdown_log(args.question, answer)
    print(f"ğŸ’¬ è³ªå•: {args.question}\nğŸ’¡ å›ç­”: {answer}")

if __name__ == "__main__":
    main()
