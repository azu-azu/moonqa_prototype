# app/qa.py

from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.schema.runnable import RunnableMap
from langchain.prompts import ChatPromptTemplate
from app.config import get_index_path
from app.settings import SCORE_THRESHOLD, OPENAI_MODEL
from app.logger import build_log_entry, append_qa_log
from app.filters import filter_docs_by_metadata  # â† è¿½åŠ ï¼
from app.classifier import classify_intent  # â† è¿½åŠ ï¼

MISSING_ANSWER = "ãƒ‡ãƒ¼ã‚¿ã®ä¸­ã«ã€ä»Šå›ã®ç­”ãˆã¯ãªã‹ã£ãŸã¿ãŸã„ã‚„ã‚ã€‚ã”ã‚ã‚“ã‚„ã§ğŸŒ™"

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆé–¢è¥¿å¼ãƒ»å¥ç‚¹æ”¹è¡Œã‚¹ã‚¿ã‚¤ãƒ«ï¼‰
PROMPT_TEMPLATE = ChatPromptTemplate.from_messages([
    (
        "system",
        "ã‚ãªãŸã¯ã€é–¢è¥¿å¼ã®ã‚„ã‚ã‚‰ã‹ã„èªã‚Šå£ã§è©±ã™ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n"
        "å£èª¿ã¯ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§ã€å„ªã—ãå¯„ã‚Šæ·»ã†ã‚ˆã†ã«è©±ã—ã¦ãã ã•ã„ã€‚\n"
        "æ–‡ä½“ã¯ã€ã§ã™ãƒ»ã¾ã™èª¿ã€ã¯ä½¿ã‚ãšã€å¥ç‚¹ï¼ˆã€‚ï¼‰ã§æ”¹è¡Œã—ã¦ãã ã•ã„ã€‚\n"
        "èª­ç‚¹ï¼ˆã€ï¼‰ã§ã¯æ”¹è¡Œã—ãªã„ã§ãã ã•ã„ã€‚\n"
        "è©©çš„ã™ãã‚‹è¡¨ç¾ã¯é¿ã‘ã€ãµã‚“ã‚ã‚Šã—ãŸé–¢è¥¿å¼ã§ã€ãƒªã‚ºãƒ ã‚ˆã3ã€œ7è¡Œã§ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚\n"
    ),
    (
        "human",
        "{context}\n\nè³ªå•: {question}"
    )
])

def load_vectorstore():
    """
    FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚
    """
    embedding = OpenAIEmbeddings()
    return FAISS.load_local(get_index_path(), embedding, allow_dangerous_deserialization=True)

def retrieve_relevant_docs(vectorstore, query):
    """
    è³ªå•ã«å¯¾ã—ã¦é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ã™ã‚‹ï¼ˆãƒ•ã‚£ãƒ«ã‚¿é©ç”¨å‰ï¼‰ã€‚
    """
    search_kwargs = {"k": 5}
    return vectorstore.similarity_search_with_score(query, **search_kwargs)

def format_docs(docs):
    """
    ãƒãƒ£ãƒ³ã‚¯ã‚’æ–‡å­—åˆ—ã¨ã—ã¦çµåˆ
    """
    return "\n\n".join(doc.page_content for doc in docs)

def get_answer(question, vectorstore, target_pdf=None):
    """
    æŒ‡å®šã•ã‚ŒãŸè³ªå•ã«å¯¾ã—ã¦å›ç­”ã‚’è¿”ã™ã€‚
    - target_pdf ã‚’æŒ‡å®šã™ã‚‹ã¨ã€è©²å½“ã™ã‚‹sourceã‚’å«ã‚€ãƒãƒ£ãƒ³ã‚¯ã®ã¿ã‚’å¯¾è±¡ã«ã™ã‚‹ã€‚
    """
    intent = classify_intent(question)  # â† intent ã‚’åˆ†é¡ï¼

    # é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
    docs_and_scores = retrieve_relevant_docs(vectorstore, question)

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨
    filter_dict = {}
    if target_pdf:
        filter_dict["source"] = target_pdf
    docs_and_scores = filter_docs_by_metadata(docs_and_scores, filter_dict)

    # ã‚¹ã‚³ã‚¢ã—ãã„å€¤ã§ã•ã‚‰ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    docs_and_scores = [(doc, score) for doc, score in docs_and_scores if score <= SCORE_THRESHOLD]

    # å›ç­”ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆ
    if not docs_and_scores:
        log_entry = build_log_entry(
            question=question,
            answer="",
            results=[],
            status="notfound",
            intent=intent  # â† intentã‚’ãƒ­ã‚°ã«è¨˜éŒ²
        )
        append_qa_log(log_entry)
        return MISSING_ANSWER, []

    docs = [doc for doc, _ in docs_and_scores]
    context = format_docs(docs)

    chain = RunnableMap({
        "context": lambda _: context,
        "question": lambda _: question
    }) | PROMPT_TEMPLATE | ChatOpenAI(model=OPENAI_MODEL, temperature=0)

    result = chain.invoke({})
    sources = ", ".join(sorted(set(doc.metadata.get("source", "?") for doc in docs)))
    full_answer = result.content + "\n\nå‚ç…§å…ƒï¼š" + sources

    log_entry = build_log_entry(
        question=question,
        answer=full_answer,
        results=[{
            "source": doc.metadata.get("source"),
            "score": float(score) # float32 â†’ float ã«å¤‰æ›
        } for doc, score in docs_and_scores],
        status="success",
        intent=intent  # â† æˆåŠŸæ™‚ã‚‚ intent ã‚’è¨˜éŒ²
    )
    append_qa_log(log_entry)

    return full_answer, docs_and_scores