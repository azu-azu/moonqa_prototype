# scripts/build_vectorstore.py

# âœ… æœ¬ç•ªç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# PDFã‹ã‚‰ãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰ã—ã¦ä¿å­˜ã™ã‚‹
# æ–°ã—ã„PDFã‚’è¿½åŠ ãƒ»å‰Šé™¤ãƒ»ä¿®æ­£ã—ãŸã¨ãã«æ¯å›å®Ÿè¡Œã™ã‚‹

import os
import glob
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ãƒ‡ãƒ¼ã‚¿ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä¿å­˜å…ˆ
DATA_DIR = "data"
INDEX_DIR = "index/faiss_index"

def load_all_pdfs(data_dir):
    all_docs = []

    # è¤‡æ•°ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    pdf_files = glob.glob(os.path.join(data_dir, "*.pdf"))

    for pdf_path in pdf_files:
        loader = PyPDFLoader(pdf_path)
        pages = loader.load()

        file_name = os.path.basename(pdf_path)
        for i, page in enumerate(pages):
            # è¡¨ç¤ºç”¨ sourceï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åï¼‹ãƒšãƒ¼ã‚¸ç•ªå·ï¼‰
            page.metadata["source"] = f"{file_name} (p.{i+1})"

            # ãƒ•ã‚£ãƒ«ã‚¿ç”¨ pdf_nameï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ï¼‰
            page.metadata["pdf_name"] = file_name

        all_docs.extend(pages)

    return all_docs

def build_vectorstore(docs):
    # ãƒãƒ£ãƒ³ã‚¯è¨­å®šã¯ç”¨é€”ã«å¿œã˜ã¦èª¿æ•´å¯èƒ½
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)
    split_docs = text_splitter.split_documents(docs)

    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(split_docs, embeddings)

    os.makedirs(INDEX_DIR, exist_ok=True)
    vectorstore.save_local(INDEX_DIR)

if __name__ == "__main__":
    print("ğŸ“„ PDFã‚’èª­ã¿è¾¼ã‚“ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆä¸­...")
    docs = load_all_pdfs(DATA_DIR)
    build_vectorstore(docs)
    print("âœ… FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†ï¼")
