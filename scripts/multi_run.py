# scripts/multi_run.py

# âœ… ãƒ†ã‚¹ãƒˆç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# `questions.json` ã«è¨˜è¼‰ã•ã‚ŒãŸè¤‡æ•°ã®è³ªå•ã‚’ãƒãƒƒãƒã§å‡¦ç†ã—ã€
# å›ç­”ã¨ä½¿ç”¨ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ã‚’ Markdown / JSON ã«ä¿å­˜ã™ã‚‹
# ç²¾åº¦æ¤œè¨¼ã‚„å›å¸°ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨

import json
import os
from qa import question_answer_with_scores, append_markdown_log, append_json_log

# è¤‡æ•°ã®è³ªå•ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
with open("data/questions.json", "r", encoding="utf-8") as f:
    questions = json.load(f)

# ã‚¹ã‚³ã‚¢ã—ãã„å€¤ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰ â€»æ¤œè¨¼ã®ãŸã‚ã«â€œå…¨éƒ¨è¦‹ã›ã¦â€ç²¾åº¦ã‚„å‚¾å‘ã‚’æŠŠæ¡ã™ã‚‹æ™‚ã¯0.0ã«ã™ã‚‹
score_threshold = 0.0

# å…¨ä»¶å‡¦ç†
for i, item in enumerate(questions):
    question = item.get("question", "").strip()
    if not question:
        continue

    print(f"\n[{i+1}/{len(questions)}] è³ªå•: {question}")
    answer, docs_and_scores = question_answer_with_scores(question, score_threshold)
    append_markdown_log(question, answer, docs_and_scores)
    append_json_log(question, answer, docs_and_scores)
    print(f"ğŸ’¡ å›ç­”: {answer}")
